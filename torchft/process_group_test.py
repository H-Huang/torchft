from unittest import TestCase

import torch
from torch.distributed import TCPStore, ReduceOp
import torch.distributed as dist
from torch import nn

from torchft.process_group import (
    ProcessGroupBabyGloo,
    ProcessGroupGloo,
    ProcessGroupDummy,
    ProcessGroup,
)


class ProcessGroupTest(TestCase):
    def test_gloo(self) -> None:
        store = TCPStore(
            host_name="localhost", port=0, is_master=True, wait_for_workers=False
        )

        store_addr = f"localhost:{store.port}/prefix"
        pg = ProcessGroupGloo()
        pg.configure(store_addr, 0, 1)

        self.assertEqual(pg.size(), 1)

        at = torch.tensor([2])

        a_work = pg.allreduce([at], ReduceOp.SUM)
        a_work.wait()

        m = nn.Linear(3, 4)
        m = torch.nn.parallel.DistributedDataParallel(m, process_group=pg)
        m(torch.rand(2, 3))

    def test_baby_gloo(self) -> None:
        store = TCPStore(
            host_name="localhost", port=0, is_master=True, wait_for_workers=False
        )

        store_addr = f"localhost:{store.port}/prefix"

        a = ProcessGroupBabyGloo()
        b = ProcessGroupBabyGloo()

        a.configure(store_addr, 0, 2)
        b.configure(store_addr, 1, 2)

        self.assertEqual(a.size(), 2)

        at = torch.tensor([1])
        bt = torch.tensor([2])

        a_work = a.allreduce([at], ReduceOp.SUM)
        b_work = b.allreduce([bt], ReduceOp.SUM)

        a_work.wait()
        b_work.wait()

        torch.testing.assert_close(at, bt)

    def test_dummy(self) -> None:
        pg = ProcessGroupDummy(0, 1)
        m = nn.Linear(3, 4)
        m = torch.nn.parallel.DistributedDataParallel(m, process_group=pg)
        m(torch.rand(2, 3))
